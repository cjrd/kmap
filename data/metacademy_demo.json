[{"title":"optimization problems","id":"optimization_problems","flags":[],"goals":[],"pointers":[{"items":[{"text":""},{"text":"Lagrange multipliers","link":"lagrange_multipliers"},{"text":" can be used to solve optimization problems with equality or inequality constraints."}],"depth":1},{"items":[{"text":"Some examples of optimization problems:"}],"depth":1},{"items":[{"text":"Fitting parameters of a statistical model using "},{"text":"maximum likelihood","link":"maximum_likelihood"}],"depth":2},{"items":[{"text":""},{"text":"Linear least squares","link":"linear_least_squares"}],"depth":2},{"items":[{"text":""},{"text":"Convex optimization problems","link":"convex_optimization"},{"text":" are a very broad class of optimization problems for which we can often find a global optimimum."}],"depth":1}],"x":830,"y":-226,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"ujcrndcb","summary":"In an optimization problem, one is interested in minimizing or maximizing a function, possibly subject to equality or inequality constraints. The extrema must occur on the boundary of the set, at points which are not differentiable, or at points where the partial derivatives are zero. \n","time":1.31543468933,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"partial_derivatives","id":"partial_derivativesoptimization_problems","reason":"Partial derivatives are necessary to characterize the critical points.","middlePts":[{"x":830,"y":-291}],"isContracted":false}],"resources":[{"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","extra":["Good, quick review of functional calculus"],"free":0,"edition":"1","location":[{"text":"Appendix D (Calculus of Variations)"}],"authors":["Christopher M. Bishop"],"title":"Pattern Recognition and Machine Learning","resource_type":"textbook","node":null,"core":0,"dependencies":[],"note":[]},{"core":1,"extra":["See Section 5.1 for the statement of the Maximum Value Theorem."],"url":"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000815.html","level":"advanced undergraduate","free":0,"edition":"1","dependencies":[{"link":"linear_approximation","title":"linear approximation"}],"location":[{"text":"Section 5.2, \"Maximum/minimum problems,\" pages 202-207"}],"authors":["Theodore Shifrin"],"title":"Multivariable Mathematics","resource_type":"textbook","node":null,"note":[]},{"core":1,"level":"introductory","url":"http://www.amazon.com/Multivariable-Calculus-Edition-Henry-Edwards/dp/0130339679","title":"Multivariable Calculus","free":0,"edition":"6","location":[{"text":"Section 13.5, \"Multivariable optimization problems,\" pages 878-886"}],"authors":["C. Henry Edwards","David E. Penney"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm","title":"MIT Open Courseware: Multivariable Caclulus","free":1,"location":[{"text":"Session 28, \"Optimization problems\" ","link":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-a-functions-of-two-variables-tangent-approximation-and-optimization/session-28-optimization-problems"},{"text":"Session 29, \"Least squares\" ","link":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-a-functions-of-two-variables-tangent-approximation-and-optimization/session-29-least-squares"}],"authors":["Denis Auroux"],"resource_type":"online lectures","node":null,"edition":"","dependencies":[],"extra":[],"note":[]}],"questions":[]},{"title":"dot product","id":"dot_product","flags":[],"goals":[],"pointers":[{"items":[{"text":"The dot product can be used to define "},{"text":"matrix multiplication","link":"matrix_multiplication"},{"text":"."}],"depth":1},{"items":[{"text":""},{"text":"Inner products","link":"inner_product"},{"text":" are generalizations of the dot product to other vector spaces. "}],"depth":1},{"items":[{"text":"The dot product is used to define the Euclidean vector norm, but there are "},{"text":"other vector norms"},{"text":" as well. "}],"depth":1}],"x":400,"y":-356,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"bveqc1fo","summary":"The dot product is an operation on vectors. It is used to define notions such as vector length, perpendicular vectors, and angles between vectors.\n","time":1.15349076451,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"vectors","id":"vectorsdot_product","reason":"The dot product is defined on vectors.","middlePts":[{"x":400,"y":-421}],"isContracted":false}],"resources":[{"core":1,"level":"introductory","url":"http://math.mit.edu/linearalgebra/","title":"Introduction to Linear Algebra","free":0,"edition":"4","location":[{"text":"Section 1.2, pages 11-17"}],"authors":["Gilbert Strang"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"resource_type":"online lectures","location":[{"text":"Lectures \"Vector dot product and vector length,\" \"Proving vector dot product properties,\" \"Proof of the Cauchy-Schwartz inequality,\" \"Vector triangle inequality,\" and \"Defining the angle between vectors\" ","link":"https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/dot_cross_products/v/vector-dot-product-and-vector-length"}],"level":"introductory","url":"https://www.khanacademy.org/math/linear-algebra","title":"Khan Academy: Linear Algebra","free":1,"node":null,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000815.html","title":"Multivariable Mathematics","free":0,"edition":"1","location":[{"text":"Section 1.2, \"Dot product,\" pages 8-13"}],"authors":["Theodore Shifrin"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[]},{"title":"probability","id":"probability","flags":[],"goals":[],"pointers":[{"items":[{"text":""},{"text":"Random variables","link":"random_variables"},{"text":" are probably the most central object of probability theory."}],"depth":1},{"items":[{"text":"A more rigorous mathematical basis for probability theory is built on "},{"text":"measure theory"},{"text":". "}],"depth":1}],"x":660,"y":-486,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"i9mo2e09","summary":"Probability theory is a set of mathematical techniques for reasoning about uncertainty. Intuitively, probabilities can be thought of as describing long-run frequencies or subjective beliefs. Mathematically, a probability measure is a function on subsets of a sample space which satisfy certain axioms.\n","time":2.42760638999,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[],"resources":[{"core":1,"level":"introductory","url":"http://www.amazon.com/First-Course-Probability-8th-Edition/dp/013603313X","title":"A First Course in Probability","free":0,"edition":"7","location":[{"text":"Section 2.2, \"Sample space and events,\" pages 24-29"},{"text":" Section 2.3, \"Axioms of probability,\" pages 29-31"},{"text":" Section 2.7, \"Probability as a measure of belief,\" pages 53-54"}],"authors":["Sheldon Ross"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.amazon.com/Introduction-Probability-Theory-Applications-Edition/dp/0471257087","title":"An Introduction to Probability Theory and its Applications","free":0,"edition":"2","location":[{"text":"Chapter 1, \"The sample space,\" pages 7-24"}],"authors":["William Feller"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.amazon.com/Probability-Statistics-Edition-Morris-DeGroot/dp/0321500466","title":"Probability and Statistics","free":0,"edition":"3","location":[{"text":"Section 1.1, \"The history of probability,\" pages 1-2"},{"text":" Section 1.2, \"Interpretations of probability,\" pages 2-5"},{"text":" Section 1.3, \"Experiments and events,\" pages 5-6"},{"text":" Section 1.4, \"Set theory,\" pages 6-12"},{"text":" Section 1.6, \"Finite sample spaces,\" pages 19-22"}],"authors":["Morris H. DeGroot","Mark J. Schervish"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"resource_type":"online lectures","location":[{"text":"Lecture \"Probability, intuition, and axioms\""}],"title":"Sets, Counting, and Probability","url":"http://www.extension.harvard.edu/open-learning-initiative/sets-counting-probability","level":"introductory","free":1,"node":null,"core":0,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"resource_type":"online lectures","location":[{"text":"Lecture 1.1, \"What is probability?\""},{"text":" Lecture 1.2, \"Addition rule\""}],"title":"BerkeleyX: Introduction to Statistics: Probability","url":"https://courses.edx.org/courses/BerkeleyX/Stat2.2x/2013_April/courseware","level":"introductory","free":1,"node":null,"core":0,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"resource_type":"online lectures","location":[{"text":"Lecture sequence \"Basic probability\" ","link":"https://www.khanacademy.org/math/probability/independent-dependent-probability/basic_probability"},{"text":"Lecture sequence \"Venn diagrams and adding probabilities\" ","link":"https://www.khanacademy.org/math/probability/independent-dependent-probability/addition_rule_probability"}],"title":"Khan Academy: Probability and Statistics","url":"https://www.khanacademy.org/math/probability","level":"introductory","free":1,"node":null,"core":0,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4","extra":["This presents the measure theoretic approach to probability, but should still be accessible without significant math background."],"free":1,"location":[{"text":"Lecture 1.1, \"Measure theory: why measure theory - the Banach-Tarski paradox\" ","link":"http://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4&index=1"},{"text":"Lecture 1.2, \"Measure theory: sigma-algebras\" ","link":"http://www.youtube.com/watch?v=21a85f1YS5Q&list=PL17567A1A3F5DB5E4&index=2"},{"text":"Lecture 1.3, \"Measure theory: measures\" ","link":"http://www.youtube.com/watch?v=xlfH6Rk2GCM&list=PL17567A1A3F5DB5E4&index=3"},{"text":"Lecture 1.4, \"Measure theory: examples of measures\" ","link":"http://www.youtube.com/watch?v=TxBLRz-gNq4&list=PL17567A1A3F5DB5E4&index=4"},{"text":"Lecture 1.5, \"Measure theory: basic properties of measures\" ","link":"http://www.youtube.com/watch?v=ILXe_NsvQ6Q&list=PL17567A1A3F5DB5E4&index=5"},{"text":"Lecture 1.6, \"Measure theory: basic properties of measures (continued)\" ","link":"http://www.youtube.com/watch?v=M4fAHWvwid8&list=PL17567A1A3F5DB5E4&index=6"},{"text":"Lecture 1.7, \"Measure theory: more properties of probability measures\" ","link":"http://www.youtube.com/watch?v=t4cwYCVyQLM&list=PL17567A1A3F5DB5E4&index=7"},{"text":"Lecture 1.8, \"Measure theory: CDFs and Borel probability measures\" ","link":"http://www.youtube.com/watch?v=wV3JuVI2tLM&list=PL17567A1A3F5DB5E4&index=8"},{"text":"Lecture 1.S, \"Measure theory: summary\" ","link":"http://www.youtube.com/watch?v=upISxgopsFU&list=PL17567A1A3F5DB5E4&index=10"}],"title":"Mathematical Monk: Probability Primer","resource_type":"online lectures","node":null,"edition":"","authors":[],"dependencies":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.cengage.com/search/productOverview.do?Ntt=9780534399429&Ntk=P_Isbn13&Ns=P_CopyRight_Year|1&N=+16+4294922455+4294922413+4294966842","title":"Mathematical Statistics and Data Analysis","free":0,"edition":"3","location":[{"text":"Section 1.1, \"Introduction,\" pages 1-2"},{"text":"Section 1.2, \"Sample spaces,\" pages 2-4"},{"text":"Section 1.3, \"Probability measures,\" pages 4-6"}],"authors":["John A. Rice"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[]},{"title":"partial derivatives","id":"partial_derivatives","flags":[],"goals":[],"pointers":[{"items":[{"text":""},{"text":"Higher order partial derivatives","link":"higher_order_partial_derivatives"}],"depth":1},{"items":[{"text":"Partial derivatives can be used to compute:"}],"depth":1},{"items":[{"text":"the "},{"text":"gradient","link":"gradient"},{"text":", an operator which \"points uphill\" "}],"depth":2},{"items":[{"text":""},{"text":"directional derivatives"},{"text":", which compute the rate of change with respect to movement in a particular direction"}],"depth":2}],"x":830,"y":-356,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"m3bm96b5","summary":"\"A partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary)\" [wikipedia entry]. Intuitively, a partial derivative measures the instantaneous rate of change for a single variate in a multivariate function.","time":1.09170487689,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"functions_of_several_variables","id":"functions_of_several_variablespartial_derivatives","reason":"","middlePts":[{"x":830,"y":-421}],"isContracted":false}],"resources":[{"core":1,"resource_type":"video tutorials","location":[{"text":"Lecture sequence: partial derivatives ","link":"https://www.khanacademy.org/math/calculus/partial_derivatives_topic/partial_derivatives/v/partial-derivatives"}],"level":"introductory","url":"https://www.khanacademy.org/","title":"Khan Academy","free":1,"node":null,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"resource_type":"wiki","location":[{"text":"Article: Partial Derivatives ","link":"http://en.wikipedia.org/wiki/Partial_derivative"}],"level":"reference","url":"http://www.wikipedia.org/","title":"Wikipedia","extra":["work through the examples section"],"free":1,"node":null,"core":0,"edition":"","authors":[],"dependencies":[],"note":[]},{"resource_type":"online compendium","location":[{"text":"Partial Derivatives Entry ","link":"http://mathworld.wolfram.com/PartialDerivative.html"}],"title":"Wolfram MathWorld","url":"http://mathworld.wolfram.com","level":"reference","free":1,"node":null,"core":0,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"http://www.amazon.com/Multivariable-Calculus-Edition-Henry-Edwards/dp/0130339679","title":"Multivariable Calculus","free":0,"location":[{"text":"Section 13.4, \"Partial derivatives,\" pages 868-875"}],"authors":["C. Henry Edwards","David E. Penney"],"resource_type":"textbook","node":null,"edition":"","dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000815.html","title":"Multivariable Mathematics","free":0,"edition":"1","location":[{"text":"Section 3.1, \"Partial derivatives and directional derivatives,\" pages 81-85"}],"authors":["Theodore Shifrin"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm","title":"MIT Open Courseware: Multivariable Caclulus","free":1,"location":[{"text":"Session 26, \"Partial derivatives\" ","link":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-a-functions-of-two-variables-tangent-approximation-and-optimization/session-26-partial-derivatives"}],"authors":["Denis Auroux"],"resource_type":"online lectures","node":null,"edition":"","dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* What is the difference between partial derivatives and implicit differentiation?","node":null},{"text":"* How are partial derivatives used to define the div, grad, and curl operators?","node":null}]},{"title":"linear regression","id":"linear_regression","flags":[],"goals":[],"pointers":[{"items":[{"text":"Linear regression is a model for predicting real-valued targets. Other kinds of targets include:"}],"depth":1},{"items":[{"text":""},{"text":"binary"}],"depth":2},{"items":[{"text":""},{"text":"categorical"}],"depth":2},{"items":[{"text":""},{"text":"ordinal"},{"text":" (i.e. only the ordering of the values is significant)"}],"depth":2},{"items":[{"text":"Vanilla linear regression is prone to "},{"text":"overfitting","link":"generalization"},{"text":"."}],"depth":1},{"items":[{"text":"Some extensions which deal with overfitting include:"}],"depth":1},{"items":[{"text":""},{"text":"ridge regression","link":"ridge_regression"}],"depth":2},{"items":[{"text":""},{"text":"L1-regularized linear regression (Lasso)","link":"lasso"}],"depth":2},{"items":[{"text":""},{"text":"PCA preprocessing"}],"depth":2},{"items":[{"text":""},{"text":"Feature selection"}],"depth":2},{"items":[{"text":""},{"text":"Model selection"}],"depth":2},{"items":[{"text":"Vanilla linear regression is also sensitive to outliers. Some more robust alternatives include:"}],"depth":1},{"items":[{"text":""},{"text":"robust linear regression"}],"depth":2},{"items":[{"text":""},{"text":"support vector regression","link":"support_vector_regression"}],"depth":2},{"items":[{"text":"The closed-form solution here does not scale well to large-scale problems (more than tens of thousands of variables). For these cases, look into "},{"text":"stochastic algorithms","link":"stochastic_gradient_descent"},{"text":"."}],"depth":1},{"items":[{"text":"Not all variables of interest can be modeled as linear functions of the input variables. To model nonlinear dependencies, check out:"}],"depth":1},{"items":[{"text":""},{"text":"basis function expansions","link":"basis_function_expansions"}],"depth":2},{"items":[{"text":""},{"text":"neural networks","link":"feed_forward_neural_nets"}],"depth":2},{"items":[{"text":""},{"text":"kernel methods","link":"kernel_ridge_regression"}],"depth":2},{"items":[{"text":"Linear regression "},{"text":"can be interpreted","link":"linear_regression_as_maximum_likelihood"},{"text":" as "},{"text":"maximum likelihood estimation","link":"maximum_likelihood"},{"text":" under a Gaussian noise model."}],"depth":1}],"x":400,"y":-96,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"x6e7glql","summary":"Linear regression is an algorithm for learning to predict a real-valued ``target'' variable as a linear function of one or more real-valued ``input'' variables. It is one of the most widely used statistical learning algorithms, and with care it can be made to work very well in practice. Because it has a closed-form solution, we can exactly analyze many properties of linear regression which have no exact form for other models. This makes it a useful starting point for understanding many other statistical learning algorithms.\n","time":2.43758404321,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"matrix_multiplication","id":"matrix_multiplicationlinear_regression","reason":"Linear regression is conveniently represented in terms of matrix-vector multiplication.","middlePts":[{"x":400,"y":-161}],"isContracted":false}],"resources":[{"core":1,"level":"advanced undergraduate","url":"http://cs229.stanford.edu/materials.html","specific_url_base":"http://cs229.stanford.edu/notes/","free":1,"location":[{"text":"Chapter 1, section 1, pages 1-7 ","link":"http://cs229.stanford.edu/notes/cs229-notes1.pdf#page=1"}],"authors":["Andrew Y. Ng"],"title":"Stanford's Machine Learning lecture notes","resource_type":"lecture notes","node":null,"edition":"","dependencies":[],"extra":[],"note":[]},{"level":"graduate","url":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/","specific_url_base":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf","free":1,"edition":"2","location":[{"text":"Section 2.3.1, \"Linear models and least squares,\" pgs. 11-14 ","link":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf#page=30"}],"authors":["Trevor Hastie","Robert Tibshirani","Jerome Friedman"],"title":"The Elements of Statistical Learning","resource_type":"online textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]},{"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","title":"Pattern Recognition and Machine Learning","free":0,"edition":"1","dependencies":[],"location":[{"text":"Sections 3.1-3.1.1, pgs. 137-142"}],"authors":["Christopher M. Bishop"],"resource_type":"textbook","node":null,"core":0,"extra":[],"note":[]},{"level":"advanced undergraduate","url":"https://www.coursera.org/course/neuralnets","extra":["Click on \"Preview\" to see the videos."],"free":1,"location":[{"text":"Lecture \"Learning the weight of a linear neuron\""},{"text":"Lecture \"The error surface of a linear neuron\""}],"authors":["Geoffrey E. Hinton"],"title":"Coursera: Neural Networks for Machine Learning","resource_type":"online lectures","node":null,"core":0,"edition":"","dependencies":[],"note":[]},{"extra":["Click on \"Preview\" to see the videos."],"core":1,"level":"introductory","url":"http://class.coursera.org/ml-003/lecture/","specific_url_base":"http://class.coursera.org/ml-003/lecture/","free":1,"location":[{"text":"Lecture sequence \"Linear regression with one variable\""},{"text":"Lecture sequence \"Linear regression with multiple variables\": \"Multiple features\" up through \"Features and polynomial regression\""}],"authors":["Andrew Y. Ng"],"title":"Coursera: Machine Leaning","resource_type":"online course","node":null,"edition":"","dependencies":[],"note":[]},{"level":"graduate","url":"http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage","specific_url_base":"http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090113.pdf","free":1,"edition":"1-online","location":[{"text":"Sections 17.1, \"Introduction: fitting a straight line,\" pages 343-344 ","link":"http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090113.pdf#page=367"},{"text":"Section 17.2, \"Linear parameter models for regression,\" up through 17.2.1, \"Vector outputs,\" pgs. 344-346 ","link":"http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090113.pdf#page=368"}],"authors":["David Barber"],"title":"Bayesian Reasoning and Machine Learning","resource_type":"online textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* What is the cost function for linear regression?","node":null},{"text":"* What does it look like when you plot it with respect to one or two parameters?","node":null},{"text":"* Be aware of two methods for solving the optimization problem: gradient descent, and solving the normal equations.","node":null},{"text":"* When would you use gradient descent, and when would you solve the normal equations?","node":null}]},{"title":"vectors","id":"vectors","flags":[],"goals":[],"pointers":[{"items":[{"text":"Central linear algebra concepts which build directly on vectors include:"}],"depth":1},{"items":[{"text":""},{"text":"the dot product","link":"dot_product"}],"depth":2},{"items":[{"text":""},{"text":"matrices","link":"matrix_multiplication"}],"depth":2},{"items":[{"text":""},{"text":"vector norms"}],"depth":2},{"items":[{"text":"Using a small set of axioms, the basic properties of vectors can be generalized to other "},{"text":"vector spaces","link":"vector_spaces"},{"text":"; examples include functions and polynomials."}],"depth":1}],"x":400,"y":-486,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"4ocvc918","summary":"A vector is is a fundamental mathematical structure that is characterized by both a direction (ordering) and a magnitude. For instance, wind has both a direction (North, South-West, etc) and a magnitude (10 km/hour) and could be represented as a vector (10 km/hour South-West). A point in Euclidean space is often represented as a vector of its coordinates and is the most common type of vector encountered. More generally, a vector is an element of a vector space -- a set that is closed under scalar multiplication and vector addition. [additional note: a vector is a very general entity that takes on many forms depending on its context, for instance, in certain vector spaces a vector could be a function such as f(x) = sin(x)]  \n","time":1.09537700151,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[],"resources":[{"url":"http://mathworld.wolfram.com/Vector.html","level":"reference","resource_type":"online compendium","free":1,"title":"Wolfram MathWorld","location":"","node":null,"core":0,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"core":1,"resource_type":"online lectures","location":[{"text":"Lecture sequence \"Vectors\"  ","link":"https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/vectors"},{"text":"Lecture \"Linear combinations and spans\" ","link":"https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/linear_combinations"}],"level":"introductory","url":"https://www.khanacademy.org/math/linear-algebra","title":"Khan Academy: Linear Algebra","free":1,"node":null,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"http://math.mit.edu/linearalgebra/","title":"Introduction to Linear Algebra","free":0,"edition":"4","location":[{"text":"Section 1.1, pages 2-6"}],"authors":["Gilbert Strang"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"level":"advanced undergraduate","url":"http://linear.ups.edu/","specific_url_base":"http://linear.ups.edu/html/","free":1,"edition":"3","dependencies":[{"link":"linear_systems_as_matrices","title":"linear systems as matrices"}],"location":[{"text":"Section \"Vector operations\" ","link":"http://linear.ups.edu/html/section-VO.html"},{"text":"Section \"Linear combinations\" ","link":"http://linear.ups.edu/html/section-LC.html"},{"text":"Section \"Spanning sets\" ","link":"http://linear.ups.edu/html/section-SS.html"}],"authors":["Robert A. Beezer"],"title":"A First Course in Linear Algebra","resource_type":"textbook","node":null,"core":0,"extra":[],"note":[]},{"resource_type":"online lectures","authors":["Gilbert Strang"],"location":[{"text":"Lecture \"The geometry of linear equations\" ","link":"http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/the-geometry-of-linear-equations/"},{"text":"Lecture \"An overview of key ideas\" ","link":"http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/an-overview-of-key-ideas/"}],"level":"introductory","url":"http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/","title":"MIT Open Courseware: Linear Algebra","free":1,"node":null,"core":0,"edition":"","dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000815.html","title":"Multivariable Mathematics","free":0,"edition":"1","location":[{"text":"Section 1.1, \"Vectors in R^n\", pages 1-6"}],"authors":["Theodore Shifrin"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* Do vectors depend on a specific coordinate system?","node":null},{"text":"* Are imaginary numbers vectors?","node":null},{"text":"* Show the Euclidean space is a vector space","node":null}]},{"title":"ridge regression","id":"ridge_regression","flags":[],"goals":[],"pointers":[{"items":[{"text":"The "},{"text":"closed-form solution","link":"linear_regression_closed_form"},{"text":" to linear regression can be extended to ridge regression."}],"depth":1},{"items":[{"text":"Ridge regression is an example of "},{"text":"regularization","link":"regularization"},{"text":"."}],"depth":1},{"items":[{"text":"Ridge regression can be "},{"text":"viewed as a Bayesian model","link":"bayesian_linear_regression"},{"text":" with a Gaussian prior over the parameters. "}],"depth":1},{"items":[{"text":"The "},{"text":"number of effective parameters"},{"text":" in ridge regression is smaller than the actual number of parameters."}],"depth":1}],"x":265,"y":164,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"20pghp9d","summary":"A problem with vanilla linear regression is that it can overfit, by forcing the learned parameters to match all the idiosyncrasies of the training data. Ridge regression, or regularized linear regression, is a way of extending the cost function with a regularizer which penalizes large weights. This leads to simpler solutions and often improves generalization performance. This idea of regularization can be used to improve the generalization performance of many other statistical models as well.\n","time":1.32616721187,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"linear_regression","id":"linear_regressionridge_regression","reason":"Ridge regression is a variant on linear regression.","middlePts":[{"x":225,"y":-31},{"x":225,"y":99}],"isContracted":false},{"source":"generalization","id":"generalizationridge_regression","reason":"Ridge regression is a way of improving generalization.","middlePts":[{"x":315,"y":99}],"isContracted":false}],"resources":[{"specific_url_base":"http://class.coursera.org/ml-003/lecture/","extra":["Click on \"Preview\" to see the videos."],"url":"http://class.coursera.org/ml-003/lecture/","level":"introductory","free":1,"location":[{"text":"Lecture \"Cost function\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=6"},{"text":"Lecture \"Regularized linear regression\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=41"}],"authors":["Andrew Y. Ng"],"title":"Coursera: Machine Leaning","resource_type":"online course","node":null,"core":0,"edition":"","dependencies":[],"note":[]},{"extra":["Don't worry about deriving the closed-form solution in equation (7.33)."],"url":"http://www.cs.ubc.ca/~murphyk/MLbook/index.html","level":"graduate","free":0,"edition":"1","dependencies":[],"location":[{"text":"Section 7.5.1, pages 225-227"}],"authors":["Kevin P. Murphy"],"title":"Machine Learning: a Probabilistic Perspective","resource_type":"textbook","node":null,"core":0,"note":[]},{"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","extra":["Don't worry about deriving the closed-form solution in equation (3.28)."],"free":0,"edition":"1","location":[{"text":"Section 3.1.4, \"Regularized least squares,\" pages 144-146"}],"authors":["Christopher M. Bishop"],"title":"Pattern Recognition and Machine Learning","resource_type":"textbook","node":null,"core":0,"dependencies":[],"note":[]},{"core":1,"level":"graduate","url":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/","specific_url_base":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf","free":1,"edition":"1","dependencies":[],"location":[{"text":"Section 3.4.3, subsection \"Ridge regression,\" pages 59-64"}],"authors":["Trevor Hastie","Robert Tibshirani","Jerome Friedman"],"title":"The Elements of Statistical Learning","resource_type":"online textbook","node":null,"extra":[],"note":[]}],"questions":[{"text":"* Derive the algorithms for solving ridge regression using both gradient descent and the normal equations.","node":null},{"text":"* If you rescale all of the features by a constant, does this affect the model's predictions in linear regression?  Ridge regression?","node":null},{"text":"* If you add multiple copies of the same feature, does this change the predictions in linear regression?  Ridge regression?","node":null},{"text":"* Suppose you want to penalize some features more heavily than others?  How would you change the cost function and the update rules?","node":null}]},{"title":"matrix multiplication","id":"matrix_multiplication","flags":[],"goals":[],"pointers":[{"items":[{"text":"Common operations on matrices include:"}],"depth":1},{"items":[{"text":""},{"text":"matrix inverse","link":"matrix_inverse"},{"text":", which is useful for "},{"text":"solving systems of linear equations","link":"linear_systems_as_matrices"}],"depth":2},{"items":[{"text":""},{"text":"eigenvalues","link":"eigenvalues_and_eigenvectors"},{"text":", a central concept in many areas of science and engineering "}],"depth":2},{"items":[{"text":"the "},{"text":"singular value decomposition","link":"singular_value_decomposition"},{"text":", a canonical representation of matrices closely related to eigenvalues "}],"depth":2},{"items":[{"text":"Many properties of matrix multiplication can be generalized to "},{"text":"linear operators on vector spaces"},{"text":"."}],"depth":1}],"x":400,"y":-226,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"9p3calmt","summary":"Matrix multiplication is an operator on matrices which satisfies many of the properties of multiplication, although not commutativity. \n","time":1.83963356784,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"dot_product","id":"dot_productmatrix_multiplication","reason":"Matrix multiplication is defined in terms of the dot product.","middlePts":[{"x":400,"y":-291}],"isContracted":false}],"resources":[{"core":1,"level":"introductory","url":"http://math.mit.edu/linearalgebra/","title":"Introduction to Linear Algebra","free":0,"edition":"4","location":[{"text":"Section 2.4, up to \"Block matrices and block multiplication,\" pages 67-70"}],"authors":["Gilbert Strang"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/","title":"MIT Open Courseware: Linear Algebra","free":1,"location":[{"text":"Lecture \"Multiplication and inverse matrices\" ","link":"http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/multiplication-and-inverse-matrices/"}],"authors":["Gilbert Strang"],"resource_type":"online lectures","node":null,"edition":"","dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"https://www.khanacademy.org/math/linear-algebra","extra":["Watch the lecture sequence \"Functions and linear transformations\" if you're not used to thinking of matrices as linear transformations."],"free":1,"location":[{"text":"Lecture sequence \"Transformations and matrix multiplication\" ","link":"https://www.khanacademy.org/math/linear-algebra/matrix_transformations/composition_of_transformations"}],"title":"Khan Academy: Linear Algebra","resource_type":"online lectures","node":null,"edition":"","authors":[],"dependencies":[],"note":[]},{"level":"advanced undergraduate","url":"http://linear.ups.edu/","specific_url_base":"http://linear.ups.edu/html/","free":1,"edition":"3","location":[{"text":"Section \"Matrix operations\" ","link":"http://linear.ups.edu/html/section-MO.html"},{"text":"Section \"Matrix multiplication\" ","link":"http://linear.ups.edu/html/section-MM.html"}],"authors":["Robert A. Beezer"],"title":"A First Course in Linear Algebra","resource_type":"textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000815.html","title":"Multivariable Mathematics","free":0,"edition":"1","location":[{"text":"Section 1.4, \"Linear transformations and matrix algebra,\" up to subsection 4.2, \"The transpose,\" pages 23-36"}],"authors":["Theodore Shifrin"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[]},{"title":"generalization","id":"generalization","flags":[],"goals":[],"pointers":[{"items":[{"text":"Some techniques for estimating generalization error include:"}],"depth":1},{"items":[{"text":""},{"text":"Cross-validation","link":"cross_validation"},{"text":", a simple and widely applicable technique"}],"depth":2},{"items":[{"text":"The "},{"text":"Akaike information criterion","link":"akaike_information_criterion"},{"text":" (for probabilistic models)"}],"depth":2},{"items":[{"text":"The "},{"text":"C_p statistic"},{"text":" (for linear regression)"}],"depth":2},{"items":[{"text":"Here are some general strategies for controlling overfitting:"}],"depth":1},{"items":[{"text":""},{"text":"model selection"}],"depth":2},{"items":[{"text":""},{"text":"feature selection"}],"depth":2},{"items":[{"text":""},{"text":"regularization","link":"regularization"}],"depth":2},{"items":[{"text":"Some theoretical concepts useful for understanding generalization include:"}],"depth":1},{"items":[{"text":"For linear regression, generalization error can be determined analytically, and "},{"text":"breaks down exactly into a sum of bias and variance terms","link":"bias_variance_decomposition"},{"text":"\". This provides a useful intuition for other models as well."}],"depth":2},{"items":[{"text":""},{"text":"Probably Approximately Correct (PAC) learning","link":"pac_learning"},{"text":", which analyzes whether an algorithm usually learns a good-enough model"}],"depth":2},{"items":[{"text":""},{"text":"VC dimension","link":"vc_dimension"},{"text":", a quantity which characterizes the complexity of a continuously-parameterized model"}],"depth":2},{"items":[{"text":""},{"text":"Structural risk minimization","link":"structural_risk_minimization"},{"text":", a way of controlling overfitting by defining a nested sequence of models of increasing complexity"}],"depth":2}],"x":315,"y":34,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"0dp1y59v","summary":"When we fit a statistical model, we are interested in generalizing, i.e. making good predictions on data we haven't seen yet. We can fail at this in two ways: by underfitting (missing important structure in the data), or by overfitting (where the model is too sensitive to idiosyncrasies in the data). We can measure the generalization error of a model by training it on a ``training set'' and then evaluating it on a separate ``test set.'' Understanding the tradeoffs of model fit vs. complexity and how to measure generalization is key to getting any machine learning algorithm to work in practice.\n \n","time":1.79277209451,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"linear_regression","id":"linear_regressiongeneralization","reason":"Linear regression is an instructive example highlighting various issues of generalization.","middlePts":[{"x":315,"y":-31}],"isContracted":false}],"resources":[{"extra":["Click on \"Preview\" to see the videos."],"core":1,"level":"introductory","url":"http://class.coursera.org/ml-003/lecture/","specific_url_base":"http://class.coursera.org/ml-003/lecture/","free":1,"location":[{"text":"Lecture \"The problem of overfitting\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=39"},{"text":"Lecture \"Model selection and train/validation/test sets\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=61"},{"text":"Lecture \"Diagnosing bias and variance\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=62"}],"authors":["Andrew Y. Ng"],"title":"Coursera: Machine Leaning","resource_type":"online course","node":null,"edition":"","dependencies":[],"note":[]},{"level":"graduate","url":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/","specific_url_base":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf","free":1,"edition":"2","location":[{"text":"Section 7.2, \"Bias, variance, and model complexity,\" pgs. 219-223 ","link":"http://www-stat.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf#page=238"}],"authors":["Trevor Hastie","Robert Tibshirani","Jerome Friedman"],"title":"The Elements of Statistical Learning","resource_type":"online textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]},{"level":"graduate","url":"http://www.cs.ubc.ca/~murphyk/MLbook/index.html","title":"Machine Learning: a Probabilistic Perspective","free":0,"edition":"1","location":[{"text":"Sections 1.4.7-1.4.8, pgs. 22-24"}],"authors":["Kevin P. Murphy"],"resource_type":"textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","title":"Pattern Recognition and Machine Learning","free":0,"edition":"1","location":[{"text":"Section 1.1, pgs. 4-12"}],"authors":["Christopher M. Bishop"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* What is meant by overfitting and underfitting?  Generalization error?","node":null},{"text":"* Have an intuitive example in mind (such as polynomial regression).","node":null},{"text":"* Assuming you have enough data, how would you measure the generalization error of a model?","node":null},{"text":"* Draw a curve representing the training and test set performances as a function of the number of training examples and explain why the curves have the shapes they do.","node":null}]},{"title":"logistic regression","id":"logistic_regression","flags":[],"goals":[],"pointers":[{"items":[{"text":"Logistic regression is a "},{"text":"discriminative model","link":"generative_vs_discriminative"},{"text":"."}],"depth":1},{"items":[{"text":"Logistic regression is a kind of "},{"text":"generalized linear model","link":"generalized_linear_models"},{"text":"."}],"depth":1},{"items":[{"text":""},{"text":"Iterative reweighted least squares (IRLS)","link":"logistic_regression_irls"},{"text":" is a faster method for fitting logistic regression in low-dimensional settings."}],"depth":1},{"items":[{"text":""},{"text":"Probit regression","link":"probit_regression"},{"text":" is a related model which uses thresholded Gaussians for the observation model instead the logistic function."}],"depth":1},{"items":[{"text":"We can "},{"text":"formulate logistic regression as a Bayesian model","link":"bayesian_logistic_regression"},{"text":" to prevent overfitting and measure our confidence in the answer."}],"depth":1},{"items":[{"text":"Here are some other commonly used binary classification algorithms:"}],"depth":1},{"items":[{"text":""},{"text":"perceptron","link":"perceptron"}],"depth":2},{"items":[{"text":""},{"text":"support vector machines (SVMs)"}],"depth":2},{"items":[{"text":""},{"text":"Gaussian discriminant analysis","link":"gaussian_discriminant_analysis"}],"depth":2},{"items":[{"text":""},{"text":"naive Bayes","link":"naive_bayes"}],"depth":2},{"items":[{"text":"We would like our classifier to "},{"text":"generalize well to new data","link":"generalization"},{"text":", not just the data it's already seen."}],"depth":1},{"items":[{"text":"Some general techniques for improving generalization include:"}],"depth":1},{"items":[{"text":""},{"text":"regularization","link":"regularization"},{"text":", where overly complex solutions are penalized"}],"depth":2},{"items":[{"text":""},{"text":"model selection"}],"depth":2},{"items":[{"text":""},{"text":"feature selection"}],"depth":2},{"items":[{"text":"Not all variables of interest can be modeled as linear functions of the input variables. To model nonlinear dependencies, check out:"}],"depth":1},{"items":[{"text":""},{"text":"basis function expansions","link":"basis_function_expansions"}],"depth":2},{"items":[{"text":""},{"text":"neural networks","link":"feed_forward_neural_nets"}],"depth":2},{"items":[{"text":""},{"text":"kernel methods","link":"kernel_svm"}],"depth":2}],"x":610,"y":294,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"fyslhiko","summary":"Logistic regression is a machine learning model for binary classification, i.e. learning to classify data points into one of two categories. It's a linear model, in that the decision depends only on the dot product of a weight vector with a feature vector. This means the classification boundary can be represented as a hyperplane. It's a widely used model in its own right, and the general structure of linear-followed-by-sigmoid is a common motif in neural networks.\n","time":1.67610929033,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"binary_linear_classifiers","id":"binary_linear_classifierslogistic_regression","reason":"Logistic regression is a binary linear classifier","middlePts":[],"isContracted":false},{"source":"linear_regression_as_maximum_likelihood","id":"linear_regression_as_maximum_likelihoodlogistic_regression","reason":"Logistic regression is like linear regression, but with a different observation model.","middlePts":[],"isContracted":false},{"source":"ridge_regression","id":"ridge_regressionlogistic_regression","reason":"The logistic regression solution is usually regularized to prevent degenerate solutions.","middlePts":[],"isContracted":false},{"source":"optimization_problems","id":"optimization_problemslogistic_regression","reason":"Fitting logistic regression requires solving an optimization problem.","middlePts":[],"isContracted":false}],"resources":[{"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","title":"Pattern Recognition and Machine Learning","free":0,"edition":"1","location":[{"text":"Section 4.3.2, pages 205-206"}],"authors":["Christopher M. Bishop"],"resource_type":"textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]},{"level":"graduate","url":"http://www.cs.ubc.ca/~murphyk/MLbook/index.html","title":"Machine Learning: a Probabilistic Perspective","free":0,"edition":"1","location":[{"text":"Sections 8-8.3.2, pages 245-249"}],"authors":["Kevin P. Murphy"],"resource_type":"textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://cs229.stanford.edu/materials.html","specific_url_base":"http://cs229.stanford.edu/notes/","free":1,"location":[{"text":"Chapter 1, section 5, pages 16-19 ","link":"http://cs229.stanford.edu/notes/cs229-notes1.pdf#page=16"}],"authors":["Andrew Y. Ng"],"title":"Stanford's Machine Learning lecture notes","resource_type":"lecture notes","node":null,"edition":"","dependencies":[],"extra":[],"note":[]},{"extra":["Click on \"Preview\" to see the videos."],"core":1,"level":"introductory","url":"http://class.coursera.org/ml-003/lecture/","specific_url_base":"http://class.coursera.org/ml-003/lecture/","free":1,"location":[{"text":"Lecture \"Hypothesis representation\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=34"},{"text":"Lecture \"Decision boundary\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=35"},{"text":"Lecture \"Cost function\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=58"},{"text":"Lecture \"Simplified cost function and gradient descent\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=36"},{"text":"Lecture \"Regularized logistic regression\" ","link":"http://class.coursera.org/ml-003/lecture/view?lecture_id=41"}],"authors":["Andrew Y. Ng"],"title":"Coursera: Machine Leaning","resource_type":"online course","node":null,"edition":"","dependencies":[],"note":[]},{"core":1,"level":"graduate","url":"http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage","specific_url_base":"http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090113.pdf","free":1,"edition":"1-online","location":[{"text":"Sections 17.4, \"Linear parameter models for classification,\" up through 17.4.3, \"Avoiding overconfident classification,\" pages 350-355 ","link":"http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090113.pdf#page=374"}],"authors":["David Barber"],"title":"Bayesian Reasoning and Machine Learning","resource_type":"online textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* How does maximum likelihood justify minimizing sum-of-squares error?","node":null},{"text":"* Try deriving the maximum likelihood estimator for linear regression assuming iid Gaussian noise","node":null}]},{"title":"binary linear classifiers","id":"binary_linear_classifiers","flags":[],"goals":[],"pointers":[{"items":[{"text":"Here are some commonly used binary classification algorithms:"}],"depth":1},{"items":[{"text":""},{"text":"perceptron","link":"perceptron"}],"depth":2},{"items":[{"text":""},{"text":"logistic regression","link":"logistic_regression"}],"depth":2},{"items":[{"text":""},{"text":"support vector machines (SVMs)"}],"depth":2},{"items":[{"text":""},{"text":"Gaussian discriminant analysis","link":"gaussian_discriminant_analysis"}],"depth":2},{"items":[{"text":""},{"text":"naive Bayes","link":"naive_bayes"}],"depth":2},{"items":[{"text":"Other kinds of targets include:"}],"depth":1},{"items":[{"text":""},{"text":"real-valued","link":"linear_regression"}],"depth":2},{"items":[{"text":""},{"text":"categorical"}],"depth":2},{"items":[{"text":""},{"text":"ordinal"},{"text":" (i.e. only the ranking matters)"}],"depth":2},{"items":[{"text":"We want our classifier to "},{"text":"generalize well to new data","link":"generalization"},{"text":", not just perform well on the data it's already seen."}],"depth":1},{"items":[{"text":"Some general techniques for improving generalization include:"}],"depth":1},{"items":[{"text":""},{"text":"regularization","link":"regularization"},{"text":", where overly complex solutions are penalized"}],"depth":2},{"items":[{"text":""},{"text":"model selection"}],"depth":2},{"items":[{"text":""},{"text":"feature selection"}],"depth":2},{"items":[{"text":"Not all variables of interest can be modeled as linear functions of the input variables. To model nonlinear dependencies, check out:"}],"depth":1},{"items":[{"text":""},{"text":"basis function expansions","link":"basis_function_expansions"}],"depth":2},{"items":[{"text":""},{"text":"neural networks","link":"feed_forward_neural_nets"}],"depth":2},{"items":[{"text":""},{"text":"kernel methods","link":"kernel_svm"}],"depth":2}],"x":485,"y":34,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"gzaovo18","summary":"A linear classifier makes a classification decision for a given observation based on the value of a linear combination of the observation's features. In a ``binary'' linear classifier, the observation is classified into one of two possible classes using a linear boundary in the input feature space.","time":0.75,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"linear_regression","id":"linear_regressionbinary_linear_classifiers","reason":"Linear regression provides useful intuitions for thinking about binary linear classification.","middlePts":[{"x":485,"y":-31}],"isContracted":false}],"resources":[{"core":1,"resource_type":"online textbook","level":"advanced undergraduate","url":"https://inst.eecs.berkeley.edu/~ee127a/book/login/l_lqp_apps_class.html","title":"Optimization Models and Applications","free":1,"location":"","node":null,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","title":"Pattern Recognition and Machine Learning","free":0,"edition":"1","location":[{"text":"Section 4-intro - 4.1.1, pages 179-182"}],"authors":["Christopher M. Bishop"],"resource_type":"textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* Is it possible to create a nonlinear decision boundary in the original feature space by mapping the linear combination of original input features through a nonlinear function?","node":null},{"text":"* Is it possible to create a nonlinear decision boundary in the original feature space by mapping the original input features through a nonlinear function and then using a binary linear classifier on the resulting features?","node":null}]},{"title":"functions of several variables","id":"functions_of_several_variables","flags":[],"goals":[],"pointers":[],"x":830,"y":-486,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"kp6axa2i","summary":"Multivariable calculus deals with functions of multiple variables. For two dimensions, we can visualize these with graphs or with level sets.\n","time":1.49083248713,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[],"resources":[{"core":1,"level":"introductory","url":"http://www.amazon.com/Multivariable-Calculus-Edition-Henry-Edwards/dp/0130339679","title":"Multivariable Calculus","free":0,"edition":"6","location":[{"text":"Section 13.2, \"Functions of several variables,\" pages 850-857"}],"authors":["C. Henry Edwards","David E. Penney"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000815.html","title":"Multivariable Mathematics","free":0,"edition":"1","location":[{"text":"Section 2.1, \"Scalar- and vector-valued functions,\" pages 53-61"}],"authors":["Theodore Shifrin"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"introductory","url":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/index.htm","title":"MIT Open Courseware: Multivariable Caclulus","free":1,"location":[{"text":"Session 24, \"Functions of two variables\" ","link":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-a-functions-of-two-variables-tangent-approximation-and-optimization/session-24-functions-of-two-variables-graphs"},{"text":"Session 25, \"Level curves and contour plots\" ","link":"http://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-a-functions-of-two-variables-tangent-approximation-and-optimization/session-25-level-curves-and-contour-plots"}],"authors":["Denis Auroux"],"resource_type":"online lectures","node":null,"edition":"","dependencies":[],"extra":[],"note":[]}],"questions":[]},{"title":"linear regression as maximum likelihood","id":"linear_regression_as_maximum_likelihood","flags":[],"goals":[],"pointers":[{"items":[{"text":"Viewing linear regression as maximum likelihood estimation leads to a number of generalization, including:"}],"depth":1},{"items":[{"text":""},{"text":"Bayesian linear regression","link":"bayesian_linear_regression"}],"depth":2},{"items":[{"text":""},{"text":"Other noise models"}],"depth":2},{"items":[{"text":"Linear regression is a kind of "},{"text":"generalized linear model","link":"generalized_linear_models"},{"text":"."}],"depth":1}],"x":695,"y":34,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"okv0c3op","summary":"One way to solve a standard linear regression problem, y=w*x, is to assume the likelihood of the observed y, p(y; w*x, sigma^2) is Gaussian. This assumption means that we believe the observed values of y are a deterministic function of w*x plus some random Gaussian noise: y = w*x + e, where e is random Gaussian noise. If we assume a known sigma, the maximum likelihood estimator for w is obtained by minimizing the sum-of-squares error, Sum[(y-w*x)^2] for all y and x pairs, which has a closed form solution.","time":0.790671507977,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"linear_regression","id":"linear_regressionlinear_regression_as_maximum_likelihood","reason":"We're interpreting linear regression as maximum likelihood.","middlePts":[{"x":655,"y":-31}],"isContracted":false},{"source":"maximum_likelihood","id":"maximum_likelihoodlinear_regression_as_maximum_likelihood","reason":"We're interpreting linear regression as maximum likelihood.","middlePts":[{"x":700,"y":-31}],"isContracted":false}],"resources":[{"core":1,"level":"advanced undergraduate","url":"http://www.youtube.com/watch?v=ulZW99jsMXY&list=PLD0F06AA0D2E8FFBA&index=55","extra":["detailed derivation of maximum likelihood estimator"],"free":1,"location":[{"text":"Lecture 9.4: MLE for linear regression ","link":"http://www.youtube.com/watch?v=ulZW99jsMXY&list=PLD0F06AA0D2E8FFBA&index=55"}],"title":"Mathematical Monk: Machine Learning","resource_type":"online lectures","node":null,"edition":"","authors":[],"dependencies":[],"note":[]},{"extra":["quick summary of maximum likelihood estimator"],"core":1,"level":"advanced undergraduate","url":"http://cs229.stanford.edu/materials.html","specific_url_base":"http://cs229.stanford.edu/notes/","free":1,"location":[{"text":"Chapter 1 Section 3 ``Probabilistic interpretation\" ","link":"http://cs229.stanford.edu/notes/cs229-notes1.pdf#page=11"}],"authors":["Andrew Y. Ng"],"title":"Stanford's Machine Learning lecture notes","resource_type":"lecture notes","node":null,"edition":"","dependencies":[],"note":[]},{"level":"graduate","url":"http://research.microsoft.com/en-us/um/people/cmbishop/prml/","title":"Pattern Recognition and Machine Learning","free":0,"edition":"1","location":[{"text":"Sections 3.1.1-3.1.2, pgs. 140-144"}],"authors":["Christopher M. Bishop"],"resource_type":"textbook","node":null,"core":0,"dependencies":[],"extra":[],"note":[]}],"questions":[{"text":"* Derive the maximum likelihood estimator of the weights in linear regression assuming independent zero-mean Gaussian noise.","node":null}]},{"title":"random variables","id":"random_variables","flags":[],"goals":[],"pointers":[{"items":[{"text":"Some important properties of random variables:"}],"depth":1},{"items":[{"text":""},{"text":"expected value","link":"expectation_and_variance"},{"text":", the value it takes \"on average\""}],"depth":2},{"items":[{"text":""},{"text":"independence","link":"independent_random_variables"},{"text":", i.e. one variable's distribution not depending on the other"}],"depth":2},{"items":[{"text":""},{"text":"variance","link":"expectation_and_variance"},{"text":", how much the value tends to deviate from the mean"}],"depth":2},{"items":[{"text":""},{"text":"entropy","link":"entropy"},{"text":", a measure of the amount of uncertainty"}],"depth":2},{"items":[{"text":"Some distributions of interest:"}],"depth":1},{"items":[{"text":"the "},{"text":"binomial distribution","link":"binomial_distribution"},{"text":", which counts the number of times an event occurs between identical and independent trials"}],"depth":2},{"items":[{"text":"the "},{"text":"multinomial distribution","link":"multinomial_distribution"},{"text":", where it takes values from a discrete set, each with some probability"}],"depth":2},{"items":[{"text":"the "},{"text":"Poisson distribution","link":"poisson_distribution"},{"text":", used for modeling numbers of events that happen independently, such as earthquakes"}],"depth":2},{"items":[{"text":"the "},{"text":"exponential distribution","link":"exponential_distribution"}],"depth":2},{"items":[{"text":"the "},{"text":"Gaussian distribution","link":"gaussian_distribution"},{"text":", a \"bell curve\" which appears extremely frequently in nature and is widely used in statistical modeling"}],"depth":2}],"x":660,"y":-356,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"cj7urqv5","summary":"Random variables are the central object of probability theory. As their name implies, can be thought of as variables whose values are randomly determined. Mathematically, they are represented as functions on a sample space. \n","time":1.73330985871,"is_shortcut":0,"isNew":1,"editNote":"","dependencies":[{"source":"probability","id":"probabilityrandom_variables","reason":"Random variables are defined in terms of probability distributions.","middlePts":[{"x":660,"y":-421}],"isContracted":false}],"resources":[{"core":1,"level":"introductory","url":"http://www.amazon.com/First-Course-Probability-8th-Edition/dp/013603313X","title":"A First Course in Probability","free":0,"edition":"7","location":[{"text":"Section 4.1, \"Random variables,\" pages 132-138"},{"text":" Section 4.2, \"Discrete random variables,\" pages 138-140"},{"text":" Section 5.1, \"Introduction,\" pages 205-209"}],"authors":["Sheldon Ross"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.amazon.com/Probability-Statistics-Edition-Morris-DeGroot/dp/0321500466","title":"Probability and Statistics","free":0,"edition":"3","location":[{"text":"Section 3.1, \"Random variables and discrete distributions,\" pages 97-102"},{"text":" Section 3.2, \"Continuous distributions,\" pages 103-108"}],"authors":["Morris H. DeGroot","Mark J. Schervish"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"level":"advanced undergraduate","url":"http://www.amazon.com/Introduction-Probability-Theory-Applications-Edition/dp/0471257087","title":"An Introduction to Probability Theory and its Applications","free":0,"edition":"2","dependencies":[{"link":"conditional_probability","title":"conditional probability"}],"location":[{"text":"Section 9.1, \"Random variables,\" pages 199-207"}],"authors":["William Feller"],"resource_type":"textbook","node":null,"core":0,"extra":[],"note":[]},{"core":1,"resource_type":"online lectures","location":[{"text":"Lecture \"Random variables\" ","link":"https://www.khanacademy.org/math/probability/random-variables-topic/random_variables_prob_dist/v/random-variables"},{"text":"Lecture \"Discrete and continuous random variables\""},{"text":"Lecture \"Probability density functions\""}],"level":"introductory","url":"https://www.khanacademy.org/math/probability","title":"Khan Academy: Probability and Statistics","free":1,"node":null,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"resource_type":"online lectures","location":[{"text":"Lecture \"Random variables\""}],"title":"Sets, Counting, and Probability","url":"http://www.extension.harvard.edu/open-learning-initiative/sets-counting-probability","level":"introductory","free":1,"node":null,"core":0,"edition":"","authors":[],"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4","extra":["This uses the measure theoretic notion of probability, but should still be accessible without that background. Refer to Lecture 1.S for unfamiliar terms."],"free":1,"location":[{"text":"Lecture 3.1, \"Random variables - definition and CDF\" ","link":"http://www.youtube.com/watch?v=V0KAb_VsLOc&list=PL17567A1A3F5DB5E4&index=16"},{"text":"Lecture 3.2, \"Types of random variables\" ","link":"http://www.youtube.com/watch?v=ZylsGiWH4Jk&list=PL17567A1A3F5DB5E4&index=17"},{"text":"Lecture 3.3, \"Discrete random variables\" ","link":"http://www.youtube.com/watch?v=zqCRz-WaKGs&list=PL17567A1A3F5DB5E4&index=18"},{"text":"Lecture 3.4, \"Random variables with densities\" ","link":"http://www.youtube.com/watch?v=zap5cXlOWuA&list=PL17567A1A3F5DB5E4&index=19"}],"title":"Mathematical Monk: Probability Primer","resource_type":"online lectures","node":null,"edition":"","authors":[],"dependencies":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.cengage.com/search/productOverview.do?Ntt=9780534399429&Ntk=P_Isbn13&Ns=P_CopyRight_Year|1&N=+16+4294922455+4294922413+4294966842","title":"Mathematical Statistics and Data Analysis","free":0,"edition":"3","location":[{"text":"Section 2.1, \"Discrete random variables,\" up through 2.1.1, \"Bernoulli random variables,\" pages 35-38"},{"text":"Section 2.2, \"Continuous random variables,\" not including subsections, pages 47-49"}],"authors":["John A. Rice"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[]},{"title":"maximum likelihood","id":"maximum_likelihood","flags":[],"goals":[],"pointers":[{"items":[{"text":"Examples of maximum likelihood estimation include:"}],"depth":1},{"items":[{"text":""},{"text":"Linear regression","link":"linear_regression_as_maximum_likelihood"}],"depth":2},{"items":[{"text":""},{"text":"Logistic regression","link":"logistic_regression"}],"depth":2},{"items":[{"text":""},{"text":"Mixture of Gaussians modeling","link":"mixture_of_gaussians"}],"depth":2},{"items":[{"text":"Maximum likelihood has some desirable "},{"text":"asymptotic properties","link":"asymptotics_of_maximum_likelihood"},{"text":" such as consistency and asymptotic normality."}],"depth":1},{"items":[{"text":"The "},{"text":"Cramer-Rao bound","link":"cramer_rao_bound"},{"text":" implies that maximum likelihood estimation is asymptotically efficient."}],"depth":1},{"items":[{"text":"For many commonly used distributions, we can find the global optimum because the "},{"text":"optimization problem is convex","link":"convex_optimization"},{"text":". "}],"depth":1},{"items":[{"text":"Other methods for estimating parameters include:"}],"depth":1},{"items":[{"text":"the "},{"text":"method of moments","link":"method_of_moments"}],"depth":2},{"items":[{"text":""},{"text":"Bayesian parameter estimation","link":"bayesian_parameter_estimation"}],"depth":2},{"items":[{"text":"Maximum likelihood can be prone to "},{"text":"overfitting","link":"generalization"},{"text":" when there is not enough data to estimate the parameters."}],"depth":1},{"items":[{"text":"Some techniques for avoiding overfitting include:"}],"depth":1},{"items":[{"text":""},{"text":"Regularized maximum likelihood","link":"regularization"}],"depth":2},{"items":[{"text":""},{"text":"Bayesian parameter estimation","link":"bayesian_parameter_estimation"}],"depth":2},{"items":[{"text":""},{"text":"Model selection"}],"depth":2},{"items":[{"text":""},{"text":"Feature selection"}],"depth":2},{"items":[{"text":""},{"text":"Early stopping","link":"early_stopping"}],"depth":2},{"items":[{"text":"Some commonly used optimization algorithms for maximum likelihood estimation include:"}],"depth":1},{"items":[{"text":""},{"text":"gradient descent","link":"gradient_descent"}],"depth":2},{"items":[{"text":""},{"text":"expectation-maximization (EM)","link":"expectation_maximization"}],"depth":2},{"items":[{"text":""},{"text":"second-order methods"}],"depth":2},{"items":[{"text":"In "},{"text":"exponential family models","link":"exponential_families"},{"text":", maximum likelihood "},{"text":"can be equivalently interpreted","link":"maximum_likelihood_in_exponential_families"},{"text":" as maximum entropy modeling."}],"depth":1}],"x":700,"y":-96,"isContracted":false,"hasContractedDeps":false,"hasContractedOLs":false,"sid":"fm218u5i","summary":"Maximum likelihood is a general and powerful technique for learning statistical models, i.e. fitting the parameters to data. The maximum likelihood parameters are the ones under which the observed data has the highest probability. It is widely used in practice, and techniques such as Bayesian parameter estimation are closely related to maximum likelihood.\n","time":1.20286411262,"is_shortcut":1,"isNew":1,"editNote":"","dependencies":[{"source":"random_variables","id":"random_variablesmaximum_likelihood","reason":"","middlePts":[{"x":660,"y":-291},{"x":660,"y":-161}],"isContracted":false},{"source":"optimization_problems","id":"optimization_problemsmaximum_likelihood","reason":"Maximum likelihood is formulated as an optimization problem.","middlePts":[{"x":785,"y":-161}],"isContracted":false}],"resources":[{"core":1,"level":"advanced undergraduate","url":"http://aima.cs.berkeley.edu/","title":"Artificial Intelligence: a Modern Approach","free":0,"edition":"2","location":[{"text":"Section 20.1, \"Statistical learning,\" pages 712-715"},{"text":" Section 20.2, \"Learning with complete data,\" up to \"Bayesian parameter learning,\" pages 716-720"}],"authors":["Stuart Russell","Peter Norvig"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"level":"advanced undergraduate","url":"http://cs229.stanford.edu/materials.html","specific_url_base":"http://cs229.stanford.edu/notes/","free":1,"dependencies":[],"location":[{"text":"Chapter 1, section 3, pages 11-13"}],"authors":["Andrew Y. Ng"],"title":"Stanford's Machine Learning lecture notes","resource_type":"lecture notes","node":null,"core":0,"edition":"","extra":[],"note":[]},{"core":1,"level":"graduate","url":"http://www.inference.phy.cam.ac.uk/itprnn/book.html","specific_url_base":"http://www.cs.toronto.edu/~mackay/itprnn/ps/","free":1,"edition":"1","dependencies":[{"link":"gaussian_distribution","title":"Gaussian distribution"}],"location":[{"text":"Section 22.1, pages 300-302"}],"authors":["David MacKay"],"title":"Information Theory, Inference, and Learning Algorithms","resource_type":"online textbook","node":null,"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.cengage.com/search/productOverview.do?Ntt=9780534399429&Ntk=P_Isbn13&Ns=P_CopyRight_Year|1&N=+16+4294922455+4294922413+4294966842","title":"Mathematical Statistics and Data Analysis","free":0,"edition":"3","location":[{"text":"Section 8.5, \"The method of maximum likelihood,\" not including subsections, pages 267-272"}],"authors":["John A. Rice"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]},{"core":1,"level":"advanced undergraduate","url":"http://www.stat.cmu.edu/~larry/all-of-statistics/","title":"All of Statistics","free":0,"edition":"1","location":[{"text":"Section 9.3, \"Maximum likelihood,\" pages 122-124"}],"authors":["Larry Wasserman"],"resource_type":"textbook","node":null,"dependencies":[],"extra":[],"note":[]}],"questions":[]}]